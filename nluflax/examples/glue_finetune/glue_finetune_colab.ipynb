{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "flax_glue_finetune.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxJl57fZUTvk"
      },
      "source": [
        "# Fine-tuning RoBERTA on GLUE tasks with Flax\n",
        "\n",
        "*By [Jason Phang](https://jasonphang.com/)*\n",
        "\n",
        "This self-contained notebook defines a RoBERTa model in Flax and shows how to fine-tune it for a GLUE task.\n",
        "\n",
        "The model design is largely based on the [Hugging Face Transformers](https://github.com/huggingface/transformers) PyTorch implementation.\n",
        "\n",
        "This was largely written as a learning exercise, and may involve suboptimal code or mistakes. If you see any improvements, please let me know!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udIj78BSRHfI"
      },
      "source": [
        "# Install the newest JAXlib version.\n",
        "!pip install --upgrade jax jaxlib==0.1.67+cuda111 -f https://storage.googleapis.com/jax-releases/jax_releases.html\n",
        "!pip install transformers datasets ml-collections\n",
        "# Install Flax at head:\n",
        "!pip install --upgrade -q git+https://github.com/google/flax.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKNug5EMXX00"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV9ATL98VJXb"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN9W0b0NRrLD"
      },
      "source": [
        "from typing import Any\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import tqdm.auto as tqdm\n",
        "import urllib\n",
        "from dataclasses import dataclass\n",
        "from sklearn.metrics import f1_score, matthews_corrcoef\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax\n",
        "from flax import linen as nn\n",
        "from flax.core import FrozenDict, freeze, unfreeze\n",
        "from flax import traverse_util\n",
        "from flax.training import train_state\n",
        "import flax.jax_utils as jax_utils\n",
        "import optax\n",
        "import torch\n",
        "import transformers\n",
        "import datasets\n",
        "from absl import flags, app\n",
        "import ml_collections"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nElVhayuVMDT"
      },
      "source": [
        "## Task-related constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX-Qon3dRyZh"
      },
      "source": [
        "NUM_LABELS_DICT = {\n",
        "    \"cola\": 2,\n",
        "    \"mnli\": 3,\n",
        "    \"mrpc\": 2,\n",
        "    \"qnli\": 2,\n",
        "    \"qqp\": 2,\n",
        "    \"rte\": 2,\n",
        "    \"sst\": 2,\n",
        "    \"stsb\": 1,\n",
        "    \"wnli\": 2,\n",
        "}\n",
        "INPUT_FIELDS_DICT = {\n",
        "    \"cola\": [\"sentence\"],\n",
        "    \"mnli\": [\"premise\", \"hypothesis\"],\n",
        "    \"mrpc\": [\"sentence1\", \"sentence2\"],\n",
        "    \"qnli\": [\"question\", \"sentence\"],\n",
        "    \"qqp\": [\"sentence1\", \"sentence2\"],\n",
        "    \"rte\": [\"sentence1\", \"sentence2\"],\n",
        "    \"sst\": [\"sentence\"],\n",
        "    \"stsb\": [\"sentence1\", \"sentence2\"],\n",
        "    \"wnli\": [\"sentence1\", \"sentence2\"],\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KXSVqH-VQ-H"
      },
      "source": [
        "## Model definition\n",
        "\n",
        "The breakdown of model layers is largely based on the Hugging Face implementation (hence why the layers are called `BertLayers` rather than RoBERTa).\n",
        "\n",
        "The one major difference is that `BertAttention` is implemented as a single module, because the `SelfAttention` module in Flax incorporates the additional output `Dense`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpJQlJ8FR-Do"
      },
      "source": [
        "@dataclass\n",
        "class RoBERTaConfig:\n",
        "    # Constant across RoBERTa\n",
        "    max_embeddings_length: int = 514\n",
        "    num_embeddings: int = 50265\n",
        "    padding_idx: int = 1\n",
        "    ln_eps: float = 1e-5\n",
        "    dropout_rate: float = 0.5\n",
        "    attention_dropout_rate: float = 0.5\n",
        "    dtype: Any = jnp.float32\n",
        "\n",
        "    # Depends on model-type\n",
        "    hidden_size: int = 768\n",
        "    intermediate_size: int = 3072\n",
        "    num_layers: int = 12\n",
        "    num_heads: int = 12\n",
        "    head_size: int = 64\n",
        "\n",
        "    # Run-dependent\n",
        "    max_seq_length: int = 256\n",
        "\n",
        "    @classmethod\n",
        "    def from_model_name(cls, model_name, max_seq_length):\n",
        "        if model_name == \"roberta-base\":\n",
        "            return cls(\n",
        "                hidden_size=768,\n",
        "                intermediate_size=3072,\n",
        "                num_layers=12,\n",
        "                num_heads=12,\n",
        "                head_size=64,\n",
        "                max_seq_length=max_seq_length,\n",
        "            )\n",
        "        elif model_name == \"roberta-large\":\n",
        "            return cls(\n",
        "                hidden_size=1024,\n",
        "                intermediate_size=4096,\n",
        "                num_layers=24,\n",
        "                num_heads=16,\n",
        "                head_size=64,\n",
        "                max_seq_length=max_seq_length,\n",
        "            )\n",
        "        else:\n",
        "            raise KeyError(model_name)\n",
        "\n",
        "class RoBERTaEmbeddings(nn.Module):\n",
        "    config: RoBERTaConfig\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, input_ids, deterministic=True):\n",
        "        batch_size = input_ids.shape[0]\n",
        "        # Token Embeddings\n",
        "        tok_embeds = nn.Embed(\n",
        "            num_embeddings=self.config.num_embeddings,\n",
        "            features=self.config.hidden_size,\n",
        "            name='embed',\n",
        "        )(input_ids)\n",
        "\n",
        "        # Pos Embeddings\n",
        "        raw_pos_ids = jnp.tile(\n",
        "            jnp.arange(self.config.max_seq_length), (batch_size, 1)\n",
        "        ).astype(jnp.int32) + (self.config.padding_idx + 1)\n",
        "        is_padding = (input_ids == self.config.padding_idx).astype(jnp.int32)\n",
        "        raw_pos_ids = raw_pos_ids * (1 - is_padding)\n",
        "        pos_ids = (raw_pos_ids + is_padding * self.config.padding_idx)\n",
        "        pos_embeds = nn.Embed(\n",
        "            num_embeddings=self.config.max_embeddings_length,\n",
        "            features=self.config.hidden_size,\n",
        "            name='pos',\n",
        "        )(pos_ids)\n",
        "\n",
        "        # Token-type Embeddings\n",
        "        tok_type_ids = jnp.zeros(self.config.max_seq_length).reshape(1, -1).astype(jnp.int32)\n",
        "        tok_type_embeds = nn.Embed(\n",
        "            num_embeddings=1,\n",
        "            features=self.config.hidden_size,\n",
        "            name='tok_type'\n",
        "        )(tok_type_ids)\n",
        "\n",
        "        # Combine, layer-norm, dropout\n",
        "        embeddings = tok_embeds + pos_embeds + tok_type_embeds\n",
        "        embeddings = nn.LayerNorm(epsilon=self.config.ln_eps)(embeddings)\n",
        "        embeddings = nn.Dropout(rate=self.config.dropout_rate)(embeddings, deterministic)\n",
        "        return embeddings\n",
        "\n",
        "class BertAttention(nn.Module):\n",
        "    config: RoBERTaConfig\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, hidden_states, attention_mask, deterministic=True):\n",
        "        att_mask = attention_mask[:, None, None, :]\n",
        "        att_output = nn.SelfAttention(\n",
        "            num_heads=self.config.num_heads,\n",
        "            dtype=self.config.dtype,\n",
        "            qkv_features=self.config.hidden_size,\n",
        "            out_features=self.config.hidden_size,\n",
        "            kernel_init=nn.initializers.xavier_uniform(),\n",
        "            bias_init=nn.initializers.normal(stddev=1e-6),\n",
        "            use_bias=True,\n",
        "            broadcast_dropout=False,\n",
        "            dropout_rate=self.config.attention_dropout_rate,\n",
        "            deterministic=deterministic,\n",
        "            name=\"self\",\n",
        "        )(hidden_states, att_mask)\n",
        "        att_output = nn.Dropout(rate=self.config.dropout_rate)(att_output, deterministic)\n",
        "        hidden_states = nn.LayerNorm(epsilon=self.config.ln_eps)(att_output + hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "class BertIntermediate(nn.Module):\n",
        "    config: RoBERTaConfig\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, hidden_states):\n",
        "        hidden_states = nn.Dense(self.config.intermediate_size)(hidden_states)\n",
        "        hidden_states = nn.gelu(hidden_states, approximate=False)\n",
        "        return hidden_states\n",
        "\n",
        "class BertOutput(nn.Module):\n",
        "    config: RoBERTaConfig\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, hidden_states, input_tensor, deterministic=True):\n",
        "        hidden_states = nn.Dense(self.config.hidden_size)(hidden_states)\n",
        "        hidden_states = nn.Dropout(rate=self.config.dropout_rate)(hidden_states, deterministic)\n",
        "        hidden_states = nn.LayerNorm(epsilon=self.config.ln_eps)(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "class BertLayer(nn.Module):\n",
        "    config: RoBERTaConfig\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, hidden_states, attention_mask, deterministic=True):\n",
        "        self_attention_outputs = BertAttention(\n",
        "            config=self.config,\n",
        "            name=\"attention\"\n",
        "        )(\n",
        "            hidden_states, attention_mask,\n",
        "        )\n",
        "        intermediate_output = BertIntermediate(\n",
        "            config=self.config,\n",
        "            name=\"intermediate\",\n",
        "        )(self_attention_outputs)\n",
        "        layer_output = BertOutput(\n",
        "            config=self.config,\n",
        "            name=\"output\"\n",
        "        )(intermediate_output, self_attention_outputs)\n",
        "        return layer_output\n",
        "\n",
        "class BertPooler(nn.Module):\n",
        "    config: RoBERTaConfig\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, hidden_states):\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = nn.Dense(self.config.hidden_size)(first_token_tensor)\n",
        "        pooled_output = nn.tanh(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "class RoBERTaModel(nn.Module):\n",
        "    config: RoBERTaConfig\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, input_ids, attention_mask, deterministic=True):\n",
        "        embeddings = RoBERTaEmbeddings(\n",
        "            config=self.config,\n",
        "            name=\"embeddings\",\n",
        "        )(input_ids)\n",
        "        hidden_states = embeddings\n",
        "        for i in range(self.config.num_layers):\n",
        "            hidden_states = BertLayer(\n",
        "                config=self.config,\n",
        "                name=f\"layer_{i}\",\n",
        "            )(\n",
        "                hidden_states=hidden_states,\n",
        "                attention_mask=attention_mask,\n",
        "                deterministic=deterministic,\n",
        "            )\n",
        "        pooled_output = BertPooler(config=self.config)(hidden_states)\n",
        "        return {\n",
        "            \"pooled\": pooled_output,\n",
        "            \"unpooled\": hidden_states,\n",
        "        }\n",
        "\n",
        "class ClassificationHead(nn.Module):\n",
        "    hidden_size: int\n",
        "    num_labels: int\n",
        "\n",
        "    dropout_rate: float = 0.5\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, pooled, deterministic=True):\n",
        "        x = nn.Dropout(self.dropout_rate)(pooled, deterministic)\n",
        "        x = nn.Dense(self.hidden_size)(x)\n",
        "        x = nn.tanh(x)\n",
        "        x = nn.Dropout(self.dropout_rate)(x, deterministic)\n",
        "        logits = nn.Dense(self.num_labels)(x)\n",
        "        return logits\n",
        "\n",
        "class RoBERTaClassificationModel(nn.Module):\n",
        "    config: RoBERTaConfig\n",
        "    num_labels: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, input_ids, attention_mask, deterministic=True):\n",
        "        encoder_outputs = RoBERTaModel(\n",
        "            config=self.config,\n",
        "            name=\"roberta\",\n",
        "        )(input_ids, attention_mask, deterministic)\n",
        "        logits = ClassificationHead(\n",
        "            hidden_size=self.config.hidden_size,\n",
        "            dropout_rate=self.config.dropout_rate,\n",
        "            num_labels=self.num_labels,\n",
        "            name=\"classification_head\",\n",
        "        )(encoder_outputs[\"pooled\"])\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhJVpWPvWFoP"
      },
      "source": [
        "## Additional model-related functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfIzlRPzSBDK"
      },
      "source": [
        "def onehot(labels, num_classes):\n",
        "    x = (labels[..., None] == jnp.arange(num_classes)[None])\n",
        "    return x.astype(jnp.float32)\n",
        "\n",
        "def cross_entropy_loss(logits, labels):\n",
        "    log_probs = nn.log_softmax(logits)\n",
        "    return -jnp.mean(jnp.sum(onehot(labels, num_classes=log_probs.shape[-1]) * log_probs, axis=-1))\n",
        "\n",
        "def from_frozen(params):\n",
        "    return {'/'.join(k): v for k, v in traverse_util.flatten_dict(params).items()}\n",
        "\n",
        "def to_frozen(flat_params):\n",
        "    return FrozenDict(traverse_util.unflatten_dict({tuple(k.split('/')): v for k, v in flat_params.items()}))\n",
        "\n",
        "def learning_rate_scheduler(lr, total_steps):\n",
        "    def f(step):\n",
        "        return lr * (total_steps - step) / total_steps\n",
        "    return f\n",
        "\n",
        "def save_params(params, path):\n",
        "    with open(path, \"wb\") as f:\n",
        "        f.write(flax.serialization.to_bytes(params))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD7wjU0IWK8K"
      },
      "source": [
        "## Functions for loading pretrained RoBERTa weights\n",
        "\n",
        "We need to do some surgery on the weights because we are using a Hugging Face/PyTorch set of weights which has a different saving convention."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvnRuuEeSJ7z"
      },
      "source": [
        "def load_params_from_pt_weights(pt_weights_path, config):\n",
        "    weights = {k: v.numpy() for k, v in torch.load(pt_weights_path).items()}\n",
        "    loaded_params = {}\n",
        "    loaded_params[f\"embeddings/embed/embedding\"] = weights['roberta.embeddings.word_embeddings.weight']\n",
        "    loaded_params[f\"embeddings/pos/embedding\"] = weights['roberta.embeddings.position_embeddings.weight']\n",
        "    loaded_params[f\"embeddings/tok_type/embedding\"] = weights['roberta.embeddings.token_type_embeddings.weight']\n",
        "    loaded_params[f\"embeddings/LayerNorm_0/bias\"] = weights[\"roberta.embeddings.LayerNorm.bias\"]\n",
        "    loaded_params[f\"embeddings/LayerNorm_0/scale\"] = weights[\"roberta.embeddings.LayerNorm.weight\"]\n",
        "    for i in range(config.num_layers):\n",
        "        loaded_params[f\"layer_{i}/attention/LayerNorm_0/bias\"] = weights[f\"roberta.encoder.layer.{i}.attention.output.LayerNorm.bias\"]\n",
        "        loaded_params[f\"layer_{i}/attention/LayerNorm_0/scale\"] = weights[f\"roberta.encoder.layer.{i}.attention.output.LayerNorm.weight\"]\n",
        "        loaded_params[f\"layer_{i}/attention/self/key/bias\"] = weights[f\"roberta.encoder.layer.{i}.attention.self.key.bias\"].reshape(config.num_heads, config.head_size)\n",
        "        loaded_params[f\"layer_{i}/attention/self/key/kernel\"] = weights[f\"roberta.encoder.layer.{i}.attention.self.key.weight\"].T.reshape(config.hidden_size, config.num_heads, config.head_size)\n",
        "        loaded_params[f\"layer_{i}/attention/self/out/bias\"] = weights[f\"roberta.encoder.layer.{i}.attention.output.dense.bias\"]\n",
        "        loaded_params[f\"layer_{i}/attention/self/out/kernel\"] = weights[f\"roberta.encoder.layer.{i}.attention.output.dense.weight\"].T.reshape(config.num_heads, config.head_size, config.hidden_size)\n",
        "        loaded_params[f\"layer_{i}/attention/self/query/bias\"] = weights[f\"roberta.encoder.layer.{i}.attention.self.query.bias\"].reshape(config.num_heads, config.head_size)\n",
        "        loaded_params[f\"layer_{i}/attention/self/query/kernel\"] = weights[f\"roberta.encoder.layer.{i}.attention.self.query.weight\"].T.reshape(config.hidden_size, config.num_heads, config.head_size)\n",
        "        loaded_params[f\"layer_{i}/attention/self/value/bias\"] = weights[f\"roberta.encoder.layer.{i}.attention.self.value.bias\"].reshape(config.num_heads, config.head_size)\n",
        "        loaded_params[f\"layer_{i}/attention/self/value/kernel\"] = weights[f\"roberta.encoder.layer.{i}.attention.self.value.weight\"].T.reshape(config.hidden_size, config.num_heads, config.head_size)\n",
        "        loaded_params[f\"layer_{i}/intermediate/Dense_0/bias\"] = weights[f\"roberta.encoder.layer.{i}.intermediate.dense.bias\"]\n",
        "        loaded_params[f\"layer_{i}/intermediate/Dense_0/kernel\"] = weights[f\"roberta.encoder.layer.{i}.intermediate.dense.weight\"].T\n",
        "        loaded_params[f\"layer_{i}/output/Dense_0/bias\"] = weights[f\"roberta.encoder.layer.{i}.output.dense.bias\"]\n",
        "        loaded_params[f\"layer_{i}/output/Dense_0/kernel\"] = weights[f\"roberta.encoder.layer.{i}.output.dense.weight\"].T\n",
        "        loaded_params[f\"layer_{i}/output/LayerNorm_0/bias\"] = weights[f\"roberta.encoder.layer.{i}.output.LayerNorm.bias\"]\n",
        "        loaded_params[f\"layer_{i}/output/LayerNorm_0/scale\"] = weights[f\"roberta.encoder.layer.{i}.output.LayerNorm.weight\"]\n",
        "    loaded_params[\"BertPooler_0/Dense_0/kernel\"] = weights[\"roberta.pooler.dense.weight\"].T\n",
        "    loaded_params[\"BertPooler_0/Dense_0/bias\"] = weights[\"roberta.pooler.dense.bias\"]\n",
        "    loaded_params = jax.device_put(to_frozen(loaded_params))\n",
        "    return loaded_params\n",
        "\n",
        "def insert_roberta_params(params, roberta_params):\n",
        "    params = unfreeze(params)\n",
        "    params[\"roberta\"] = roberta_params\n",
        "    params = freeze(params)\n",
        "    return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwrGJBfeWZv7"
      },
      "source": [
        "## Data-related functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LORiVlrmSMIH"
      },
      "source": [
        "def prepare_dataset(model_name, task, max_seq_length):\n",
        "    task_dataset = datasets.load_dataset(\"glue\", name=task)\n",
        "    tokenizer = transformers.RobertaTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def tokenize_examples(examples: dict):\n",
        "        return tokenizer(\n",
        "            *[examples[field] for field in INPUT_FIELDS_DICT[task]],\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_seq_length,\n",
        "            truncation=\"longest_first\",\n",
        "        )\n",
        "    tokenized_dataset = task_dataset.map(tokenize_examples, batched=True)\n",
        "    if task == \"mnli\":\n",
        "        return {\n",
        "            \"train\": tokenized_dataset[\"train\"],\n",
        "            \"validation\": tokenized_dataset[\"validation_matched\"],\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            \"train\": tokenized_dataset[\"train\"],\n",
        "            \"validation\": tokenized_dataset[\"validation\"],\n",
        "        }\n",
        "\n",
        "def convert_batch(raw_batch, reshape_for_devices=True):\n",
        "    batch_size = len(raw_batch[\"input_ids\"][0])\n",
        "    local_device_count = jax.local_device_count()\n",
        "    full_batch =  {\n",
        "        \"input_ids\": np.array(raw_batch[\"input_ids\"]),\n",
        "        \"attention_mask\": np.array(raw_batch[\"attention_mask\"]),\n",
        "        \"label\": np.array(raw_batch[\"label\"]),\n",
        "    }\n",
        "    if reshape_for_devices:\n",
        "        batch = {\n",
        "            k: v.reshape((local_device_count, -1) + v.shape[1:])\n",
        "            for k, v in full_batch.items()\n",
        "        }\n",
        "        return batch\n",
        "    else:\n",
        "        return full_batch\n",
        "\n",
        "def compute_batch_metrics(logits, labels, task):\n",
        "    if task in [\"qnli\", \"mnli\", \"rte\", \"sst\", \"wnli\"]:\n",
        "        metrics = score_task(logits, labels, task)\n",
        "    else:\n",
        "        # The other metrics aren't jax-friendly\n",
        "        metrics = {}\n",
        "    metrics[\"loss\"] = cross_entropy_loss(logits, labels)\n",
        "    return metrics\n",
        "\n",
        "def score_task(logits, labels, task):\n",
        "    if task in [\"qnli\", \"mnli\", \"rte\", \"sst\", \"wnli\"]:\n",
        "        preds = logits.argmax(-1)\n",
        "        return {\n",
        "            \"accuracy\": (preds == labels).mean(),\n",
        "        }\n",
        "    elif task == \"stsb\":\n",
        "        preds = logits[:, -1]\n",
        "        pearson_corr = pearsonr(preds, labels)[0]\n",
        "        spearman_corr = spearmanr(preds, labels)[0]\n",
        "        return {\n",
        "            \"pearson\": pearson_corr,\n",
        "            \"spearmanr\": spearman_corr,\n",
        "            \"corr\": (pearson_corr + spearman_corr) / 2,\n",
        "        }\n",
        "    elif task in [\"mrpc\", \"qqp\"]:\n",
        "        preds = logits.argmax(-1)\n",
        "        acc = (preds == labels).mean()\n",
        "        labels = np.array(labels)\n",
        "        f1 = f1_score(y_true=labels, y_pred=preds)\n",
        "        return {\n",
        "            \"acc\": acc,\n",
        "            \"f1\": f1,\n",
        "            \"acc_and_f1\": (acc + f1) / 2,\n",
        "        }\n",
        "    elif task == \"cola\":\n",
        "        preds = logits.argmax(-1)\n",
        "        return {\n",
        "            \"mcc\": matthews_corrcoef(labels, preds),\n",
        "        }\n",
        "    elif task == \"sst\":\n",
        "        return\n",
        "    else:\n",
        "        raise KeyError(task)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrRo8vg6WdQu"
      },
      "source": [
        "## Train and Eval steps\n",
        "\n",
        "This is where we call `jax.jit` to make things run fast!\n",
        "\n",
        "I use a `static_argnames` here because some of the above functions differe depending on the task, and `jax.jit` needs to know about such additional arguments explicitly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1K6xJYWSPGu"
      },
      "source": [
        "def train_step(state, batch, task):\n",
        "    def loss_fn(params):\n",
        "        logits_ = state.apply_fn(\n",
        "            {\"params\": params},\n",
        "            batch[\"input_ids\"],\n",
        "            batch[\"attention_mask\"],\n",
        "            False,\n",
        "        )\n",
        "        if task == \"stsb\":\n",
        "            loss = ((logits_[:, 0] - batch[\"label\"]) ** 2).mean()\n",
        "        else:\n",
        "            loss = cross_entropy_loss(logits_, batch[\"label\"])\n",
        "        return loss, logits_\n",
        "\n",
        "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "    (_, logits), grads = grad_fn(state.params)\n",
        "    grads = jax.lax.pmean(grads, 'batch')\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    metrics = compute_batch_metrics(logits, batch['label'], task=task)\n",
        "    return state, metrics\n",
        "\n",
        "def eval_step(state, batch):\n",
        "    logits = state.apply_fn(\n",
        "        {\"params\": state.params},\n",
        "        batch[\"input_ids\"],\n",
        "        batch[\"attention_mask\"],\n",
        "        True,\n",
        "    )\n",
        "    return logits\n",
        "\n",
        "multi_device_train_step = jax.pmap(train_step, axis_name=\"batch\", static_broadcasted_argnums=(2,))\n",
        "multi_device_eval_step = jax.pmap(eval_step, axis_name=\"batch\")\n",
        "single_device_eval_step = jax.jit(eval_step)\n",
        "# multi_device_train_step = jax.jit(train_step, static_argnums=(2,))\n",
        "# multi_device_eval_step = jax.jit(eval_step)\n",
        "# single_device_eval_step = multi_device_eval_step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyOmTdLeWveS"
      },
      "source": [
        "## Train and Eval epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyCE5NE6SQ18"
      },
      "source": [
        "def train_epoch(state, train_ds, task, batch_size):\n",
        "    batch_metrics = []\n",
        "    num_examples = len(train_ds)\n",
        "    # Skip the last batch. Too annoying to deal with :/\n",
        "    num_used_examples = num_examples // batch_size * batch_size\n",
        "    permuted_idx = np.random.permutation(num_examples)[:num_used_examples]\n",
        "    for i in tqdm.trange(0, num_used_examples, batch_size):\n",
        "        batch_idx = permuted_idx[i:i+batch_size]\n",
        "        batch = convert_batch(train_ds[batch_idx])\n",
        "        state, metrics = multi_device_train_step(state, batch, task)\n",
        "        batch_metrics.append(metrics)\n",
        "    batch_metrics_np = jax.device_get(batch_metrics)\n",
        "    epoch_metrics_np = {\n",
        "        k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
        "        for k in batch_metrics_np[0]\n",
        "    }\n",
        "    return state, epoch_metrics_np\n",
        "\n",
        "def eval_model(state, validation_ds, task, eval_batch_size):\n",
        "    num_examples = len(validation_ds)\n",
        "    all_logits = []\n",
        "    for i in tqdm.trange(0, num_examples, eval_batch_size):\n",
        "        raw_batch = validation_ds[i:i+eval_batch_size]\n",
        "        if len(raw_batch[\"input_ids\"]) == eval_batch_size:\n",
        "            # Regular batch\n",
        "            batch = convert_batch(raw_batch)\n",
        "            logits = jax.device_get(multi_device_eval_step(state, batch))\n",
        "            logits = logits.reshape(-1, logits.shape[-1])\n",
        "        else:\n",
        "            # Special handling for last batch.\n",
        "            # I guess we'll just loop over it?\n",
        "            # If we really wanted to be efficient, we could do some of this last\n",
        "            # batch across all devices first.\n",
        "            local_device_count = jax.local_device_count()\n",
        "            capacity_per_device = eval_batch_size // local_device_count\n",
        "            sub_logits_ls = []\n",
        "            for j in range(0, len(raw_batch[\"input_ids\"]), capacity_per_device):\n",
        "                single_device_state = jax_utils.unreplicate(state)\n",
        "                raw_sub_batch = validation_ds[i + j: i + j+capacity_per_device]\n",
        "                sub_batch = convert_batch(\n",
        "                    raw_sub_batch, \n",
        "                    reshape_for_devices=False,\n",
        "                )\n",
        "                sub_logits = jax.device_get(\n",
        "                    single_device_eval_step(single_device_state, sub_batch)\n",
        "                )\n",
        "                sub_logits_ls.append(sub_logits)\n",
        "            logits = np.concatenate(sub_logits_ls, axis=0)\n",
        "            \n",
        "        all_logits.append(logits)\n",
        "    all_logits = np.concatenate(all_logits, axis=0)\n",
        "    all_labels = np.array(validation_ds[\"label\"])\n",
        "    return score_task(all_logits, all_labels, task)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJrI2oOGW-_4"
      },
      "source": [
        "## Run Configuration\n",
        "\n",
        "Feel free to change some of these arguments, e.g. `model_name` (roberta-base/roberta-large), or `task` (cola/mnli/mrpc/qnli/qqp/rte/sst/stsb/wnli)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoG_2VZWv2XQ"
      },
      "source": [
        "# if jax.local_devices()[0].platform == \"tpu\":\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    import jax.tools.colab_tpu\n",
        "    jax.tools.colab_tpu.setup_tpu()\n",
        "    print(\"Running on TPU\")\n",
        "    use_tpu = True\n",
        "except ValueError:\n",
        "    print(\"Not running on TPU\")\n",
        "    use_tpu = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4J2aXD1S48u"
      },
      "source": [
        "cfg = ml_collections.ConfigDict\n",
        "cfg.max_seq_length = 256\n",
        "cfg.batch_size = 64 if use_tpu else 16\n",
        "cfg.learning_rate = 1e-5\n",
        "cfg.num_epochs = 10\n",
        "cfg.model_name = \"roberta-base\"\n",
        "cfg.output_dir = \"/content/experiment\"\n",
        "cfg.task = \"stsb\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAfB97QOXSM9"
      },
      "source": [
        "# Run fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_eg2dxtSSmU"
      },
      "source": [
        "# Initial prep\n",
        "os.makedirs(cfg.output_dir, exist_ok=True)\n",
        "model_config = RoBERTaConfig.from_model_name(cfg.model_name, max_seq_length=cfg.max_seq_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_btRf4HSsXZ"
      },
      "source": [
        "# Prepare the dataset\n",
        "tokenized_dataset = prepare_dataset(\n",
        "    model_name=cfg.model_name,\n",
        "    task=cfg.task,\n",
        "    max_seq_length=cfg.max_seq_length,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQ7Ik6hGSthS"
      },
      "source": [
        "# Create our model\n",
        "rng = jax.random.PRNGKey(0)\n",
        "rng, init_rng = jax.random.split(rng)\n",
        "task_model = RoBERTaClassificationModel(\n",
        "    config=model_config,\n",
        "    num_labels=NUM_LABELS_DICT[cfg.task],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsDQgMBWSu-Q"
      },
      "source": [
        "# Create dummy inputs to help initialize parameters\n",
        "dummy_input_ids = jnp.ones([1, cfg.max_seq_length]).astype(jnp.int32)\n",
        "dummy_mask = jnp.ones([1, cfg.max_seq_length])\n",
        "params = task_model.init(init_rng, dummy_input_ids, dummy_mask)['params']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ymGLnWvSwKN"
      },
      "source": [
        "# Insert pretrained RoBERTa encoder params\n",
        "urllib.request.urlretrieve(\n",
        "    f\"https://huggingface.co/{cfg.model_name}/resolve/main/pytorch_model.bin\",\n",
        "    os.path.join(cfg.output_dir, f\"{cfg.model_name}.p\"),\n",
        ")\n",
        "roberta_params = load_params_from_pt_weights(\n",
        "    os.path.join(cfg.output_dir, f\"{cfg.model_name}.p\"),\n",
        "    config=model_config,\n",
        ")\n",
        "params = insert_roberta_params(params, roberta_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_zVB5TFSxTV"
      },
      "source": [
        "# Set up our optimizer and training state\n",
        "total_steps = cfg.num_epochs * (len(tokenized_dataset[\"train\"]) // cfg.batch_size)\n",
        "tx = optax.adamw(learning_rate_scheduler(cfg.learning_rate, total_steps=total_steps))\n",
        "state = train_state.TrainState.create(apply_fn=task_model.apply, params=params, tx=tx)\n",
        "state = jax_utils.replicate(state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA3uRkq1SyO1"
      },
      "source": [
        "# Start training\n",
        "epoch_metrics = []\n",
        "for epoch_i in tqdm.trange(cfg.num_epochs):\n",
        "    state, metrics = train_epoch(\n",
        "        state=state,\n",
        "        train_ds=tokenized_dataset[\"train\"],\n",
        "        task=cfg.task,\n",
        "        batch_size=cfg.batch_size,\n",
        "    )\n",
        "    epoch_metrics.append(metrics)\n",
        "    eval_metrics = eval_model(\n",
        "        state=state,\n",
        "        validation_ds=tokenized_dataset[\"validation\"],\n",
        "        task=cfg.task,\n",
        "        eval_batch_size=cfg.batch_size * 2,\n",
        "    )\n",
        "    print(f\"Epoch {epoch_i}\", eval_metrics)\n",
        "    save_params(params, os.path.join(f\"model__epoch{epoch_i}.params\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-oNbh7zW_1-"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkX3DPVNmibv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DH6r3ZLrnu4Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}